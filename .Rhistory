business.df <- flatten(business.df)
business.df
business.df.filtered <- business.df.filtered[!(is.na(business.df.filtered$neighborhood) | business.df.filtered$neighborhood==""), ]
business.df <- business.df.filtered
business.df
business <- "dataset/business.json"
review <-read_lines(business, n_max = 200000)
business.df <- fromJSON(paste("[", paste(review, collapse = ","), "]"))
business.df <- flatten(business.df)
business.df
business.df <- business.df[!(is.na(business.df$neighborhood) | business.df.filtered$neighborhood==""), ]
business.df
business.df <- business.df[!(is.na(business.df$neighborhood) | business.df.filtered$neighborhood==""), ]
business.df
business.df <- business.df[!(is.na(business.df$neighborhood) | business.df$neighborhood==""), ]
business.df
# one-hot-encode all the categories
library(qdapTools)
categories <- mtabulate(business.df$categories)
categories
# create a count dataframe to count the frequency of each unique category and then sum them
categories.sum <- data.frame(
category=character(),
counts=integer(),
stringsAsFactors=FALSE)
for (name in names(categories)) {
categories.sum[nrow(categories.sum) + 1,] = list(name, sum(categories[name]))
}
categories.sum
# Keep the top 15 categories
eliminated.categories <- categories.sum[order(categories.sum$counts, decreasing = TRUE),]
eliminated.categories <- eliminated.categories[1:15,]
eliminated.categories
# remove all the column that is not the top 15 category
library(dplyr)
categories.filtered <- categories[eliminated.categories$category]
categories.filtered$ID <- seq.int(nrow(categories.filtered))
business.df$ID <- seq.int(nrow(business.df))
encoded.business.df <- left_join(business.df[,c("ID", "business_id", "name", "stars")], categories.filtered)
encoded.business.df
encoded.business.df <- left_join(business.df[,c("ID", "neighbor")], categories.filtered)
encoded.business.df <- left_join(business.df[,c("ID", "neighborhood")], categories.filtered)
encoded.business.df
data("Alzheimer")
Alzheimer
encoded.business.df
factor(encoded.business.df$neighborhood)
encoded.business.df
drop <- c("ID")
encoded.business.df <- encoded.business.df[ , !(names(encoded.business.df) %in% drops)]
drops <- c("ID")
encoded.business.df <- encoded.business.df[ , !(names(encoded.business.df) %in% drops)]
encoded.business.df
factor(encoded.business.df$neighborhood)
library("jsonlite")
library("ggplot2")
library("readr")
setwd("~/Documents/Project/YelpAnalysis/")
business <- "dataset/business.json"
review <-read_lines(business, n_max = 200000)
business.df <- fromJSON(paste("[", paste(review, collapse = ","), "]"))
business.df <- flatten(business.df)
business.df
encoded.business.df$neighborhood <- factor(encoded.business.df$neighborhood)
encoded.business.df$neighborhood <- as.numeric(encoded.business.df$neighborhood)
encoded.business.df
fit_lca <- blca.em(encoded.business.df, max(encoded.business.df$neighborhood))
fit_lca <- blca.em(encoded.business.df, 10)
fit_lca <- blca.em(encoded.business.df, 2) # max(encoded.business.df$neighborhood)
plot(fit_lca)
plot(fit_lca)
fit_lca <- blca.em(encoded.business.df, 2) # max(encoded.business.df$neighborhood)
library("jsonlite")
library("ggplot2")
library("readr")
setwd("~/Documents/Project/YelpAnalysis/")
business <- "dataset/business.json"
review <-read_lines(business, n_max = 200000)
business.df <- fromJSON(paste("[", paste(review, collapse = ","), "]"))
business.df <- flatten(business.df)
business.df
business.df <- business.df[!(is.na(business.df$neighborhood) | business.df$neighborhood==""), ]
business.df
# one-hot-encode all the categories
library(qdapTools)
categories <- mtabulate(business.df$categories)
categories
# create a count dataframe to count the frequency of each unique category and then sum them
categories.sum <- data.frame(
category=character(),
counts=integer(),
stringsAsFactors=FALSE)
for (name in names(categories)) {
categories.sum[nrow(categories.sum) + 1,] = list(name, sum(categories[name]))
}
categories.sum
# Keep the top 15 categories
eliminated.categories <- categories.sum[order(categories.sum$counts, decreasing = TRUE),]
eliminated.categories <- eliminated.categories[1:15,]
eliminated.categories
# remove all the column that is not the top 15 category
library(dplyr)
categories.filtered <- categories[eliminated.categories$category]
categories.filtered
categories.filtered$ID <- seq.int(nrow(categories.filtered))
business.df$ID <- seq.int(nrow(business.df))
encoded.business.df <- left_join(business.df[,c("ID", "neighborhood")], categories.filtered)
encoded.business.df
# drop unneeded columns
drops <- c("ID")
encoded.business.df <- encoded.business.df[ , !(names(encoded.business.df) %in% drops)]
encoded.business.df
# label neighborhood
encoded.business.df$neighborhood <- factor(encoded.business.df$neighborhood)
encoded.business.df$neighborhood <- as.numeric(encoded.business.df$neighborhood)
encoded.business.df
install.packages("Mclust")
install.packages("mclust")
install.packages("mclust")
```
libraray(mclust)
library(mclust)
fit.clust <- Mclust(encoded.business.df, G=10)
plot(fit.clust)
library("jsonlite")
library("ggplot2")
library("readr")
setwd("~/Documents/Project/YelpAnalysis/")
business <- "dataset/business.json"
review <-read_lines(business, n_max = 200000)
business.df <- fromJSON(paste("[", paste(review, collapse = ","), "]"))
business.df <- flatten(business.df)
# only keep rows where business is located in toronto
business.df <- business.df[business.df$city == "Toronto",]
business.df
business.df.filtered <- business.df[,c("business_id", "neighborhood")]
business.df.filtered <- business.df.filtered[!(is.na(business.df.filtered$neighborhood) | business.df.filtered$neighborhood==""), ]
business.df.filtered
business.df.filtered <- business.df.filtered[business.df.filtered$neighborhood %in% names(table(business.df.filtered$neighborhood))[table(business.df.filtered$neighborhood) >= 1],]
business.df$neighborhood <- factor(business.df$neighborhood)
nlevels(business.df$neighborhood)
compare_m_gibbs <- function(y, ind, maxiter = 5000)
{
### weakly informative priors
a0 <- 1/2 ; b0 <- 50 ## tau_w hyperparameters, gamma
eta0 <-1/2 ; t0 <- 50 ## tau_b hyperparameters, gamma
mu0<-50 ## mean, norm
gamma0 <- 1/25
###
### starting values
m <- nlevels(ind)
ybar <- theta <- tapply(y, ind, mean)
ybar[is.na(ybar)] <- 0
tau_w <- mean(1 / tapply(y, ind, var)) ##within group precision
tau_w[is.na(tau_w)] <- 0
mu <- mean(theta)
mu[is.na(mu)] <- 0
tau_b <-var(theta) ##between group precision
tau_b[is.na(tau_b)] <- 0
n_m <- tapply(y, ind, length)
n_m[is.na(n_m)] <- 0
an <- a0 + sum(n_m)/2
###
### setup MCMC
theta_mat <- matrix(0, nrow=maxiter, ncol=m)
mat_store <- matrix(0, nrow=maxiter, ncol=3)
###
### MCMC algorithm
for(s in 1:maxiter)
{
# sample new values of the thetas
for(j in 1:m)
{
taun <- n_m[j] * tau_w + tau_b
thetan <- (ybar[j] * n_m[j] * tau_w + mu * tau_b) / taun
theta[j]<-rnorm(1, thetan, 1/sqrt(taun))
}
#sample new value of tau_w
ss <- 0
for(j in 1:m){
ss <- ss + sum((y[ind == j] - theta[j])^2)
}
bn <- b0 + ss/2
tau_w <- rgamma(1, an, bn)
#sample a new value of mu
gammam <- m * tau_b + gamma0
mum <- (mean(theta) * m * tau_b + mu0 * gamma0) / gammam
mu <- rnorm(1, mum, 1/ sqrt(gammam))
# sample a new value of tau_b
etam <- eta0 + m/2
tm <- t0 + sum((theta-mu)^2)/2
tau_b <- rgamma(1, etam, tm)
#store results
theta_mat[s,] <- theta
mat_store[s, ] <- c(mu, tau_w, tau_b)
}
colnames(mat_store) <- c("mu", "tau_w", "tau_b")
return(list(params = mat_store, theta = theta_mat))
}
fit <- compare_m_gibbs(business.df$stars, business.df$neighborhood)
mean <-apply(fit$params, 2, mean)
result <- apply(fit$params, 2, mean)
max(fit$params[,1] - result[1])
fit
fit <- compare_m_gibbs(business.df$stars, business.df$neighborhood)
mean <-apply(fit$params, 2, mean)
fit
business <- stream_in(file( "dataset/business.json"))
business <- stream_in(file( "dataset/business.json"))
business.df <- flatten(business.df)
review <- stream_in(file("dataset/review_Toronto.json"))
review <- stream_in(file("dataset/review_Toronto.json"))
review.df <- flatten(review.df)
review.df
review.df <- review.df[,c("business_id", "user_id", "stars")]
review.df$neighborhood <- business.df$neighborhood [match(review.df$business_id, business.df$business_id)]
review.df
business.df[business.df$business_id == "0W4lkclzZThpx3V65bVgig",]
business.df[business.df$business_id == "0W4lkclzZThpx3V65bVgig",]
business.df[,business.df$business_id == "0W4lkclzZThpx3V65bVgig"]
0W4lkclzZThpx3V65bVgig
business.df
subset(business.df, business_id=='l09JfMeQ6ynYs5MCJtrcmQ')
0W4lkclzZThpx3V65bVgig
review.df
review.df <- review.df[!(is.na(review.df) | review.df==""), ]
# Import business dataset
business <- stream_in(file( "dataset/business.json"))
business.df <- flatten(business.df)
business.df
# Import review dataset
review <- stream_in(file("dataset/review_Toronto.json"))
review.df <- flatten(review.df)
# only keep stars, businees id and user id in the review dataset
review.df <- review.df[,c("business_id", "user_id", "stars")]
review.df$neighborhood <- business.df$neighborhood [match(review.df$business_id, business.df$business_id)]
review.df
# drop null or empty neighborhood
review.df <- review.df[!(is.na(review.df) | review.df==""), ]
review.df
review.df <- review.df[review.df$neighborhood %in% names(table(review.df$neighborhood))[table(review.df$neighborhood) >= 1],]
review.df
review.df$neighborhood <- factor(review.df$neighborhood)
nlevels(review.df$neighborhood)
compare_m_gibbs <- function(y, ind, maxiter = 5000)
{
### weakly informative priors
a0 <- 1/2 ; b0 <- 50 ## tau_w hyperparameters, gamma
eta0 <-1/2 ; t0 <- 50 ## tau_b hyperparameters, gamma
mu0<-50 ## mean, norm
gamma0 <- 1/25
###
### starting values
m <- nlevels(ind)
ybar <- theta <- tapply(y, ind, mean)
ybar[is.na(ybar)] <- 0
tau_w <- mean(1 / tapply(y, ind, var)) ##within group precision
tau_w[is.na(tau_w)] <- 0
mu <- mean(theta)
mu[is.na(mu)] <- 0
tau_b <-var(theta) ##between group precision
tau_b[is.na(tau_b)] <- 0
n_m <- tapply(y, ind, length)
n_m[is.na(n_m)] <- 0
an <- a0 + sum(n_m)/2
###
### setup MCMC
theta_mat <- matrix(0, nrow=maxiter, ncol=m)
mat_store <- matrix(0, nrow=maxiter, ncol=3)
###
### MCMC algorithm
for(s in 1:maxiter)
{
# sample new values of the thetas
for(j in 1:m)
{
taun <- n_m[j] * tau_w + tau_b
thetan <- (ybar[j] * n_m[j] * tau_w + mu * tau_b) / taun
theta[j]<-rnorm(1, thetan, 1/sqrt(taun))
}
#sample new value of tau_w
ss <- 0
for(j in 1:m){
ss <- ss + sum((y[ind == j] - theta[j])^2)
}
bn <- b0 + ss/2
tau_w <- rgamma(1, an, bn)
#sample a new value of mu
gammam <- m * tau_b + gamma0
mum <- (mean(theta) * m * tau_b + mu0 * gamma0) / gammam
mu <- rnorm(1, mum, 1/ sqrt(gammam))
# sample a new value of tau_b
etam <- eta0 + m/2
tm <- t0 + sum((theta-mu)^2)/2
tau_b <- rgamma(1, etam, tm)
#store results
theta_mat[s,] <- theta
mat_store[s, ] <- c(mu, tau_w, tau_b)
}
colnames(mat_store) <- c("mu", "tau_w", "tau_b")
return(list(params = mat_store, theta = theta_mat))
}
fit <- compare_m_gibbs(business.df$stars, business.df$neighborhood)
fit
ggplot(review.df) + geom_boxplot(aes(x = reorder(neighborhood, stars, median), stars, fill = reorder(neighborhood, stars, median)), show.legend=FALSE)
ggplot(review.df) + geom_boxplot(aes(x = reorder(neighborhood, stars, median), stars, fill = reorder(neighborhood, stars, median)), show.legend=FALSE)
fit$theta
class(fit)
fit$theta
nlevels(review.df$neighborhood)
fit <- compare_m_gibbs(review.df$stars, review.df$neighborhood)
# Import business dataset
business <- stream_in(file( "dataset/business.json"))
business.df <- flatten(business.df)
business.df
# Import review dataset
review <- stream_in(file("dataset/review_Toronto.json"))
review.df <- flatten(review.df)
# only keep stars, businees id and user id in the review dataset
review.df <- review.df[,c("business_id", "user_id", "stars")]
review.df$categories <- business.df$categories [match(review.df$business_id, business.df$business_id)]
review.df
review.df <- review.df[!(is.na(review.df) | review.df==""), ]
# drop null or empty neighborhood
review.df <- review.df[!(is.na(review.df) | review.df==""), ]
review.df
# Import business dataset
business <- stream_in(file( "dataset/business.json"))
business.df <- flatten(business.df)
business.df
library("jsonlite")
library("ggplot2")
library("readr")
setwd("~/Documents/Project/YelpAnalysis/")
# Import business dataset
business <- stream_in(file( "dataset/business.json"))
business.df <- flatten(business.df)
business.df
library("jsonlite")
library("ggplot2")
library("readr")
setwd("~/Documents/Project/YelpAnalysis/")
# Import business dataset
business <- stream_in(file( "dataset/business.json"))
business.df <- flatten(business)
business.df
library("jsonlite")
library("ggplot2")
library("readr")
setwd("~/Documents/Project/YelpAnalysis/")
library("jsonlite")
library("ggplot2")
library("readr")
setwd("~/Documents/Project/YelpAnalysis/")
# Import business dataset
business <- stream_in(file( "dataset/business.json"))
business.df <- flatten(business)
business.df
# Import review dataset
review <- stream_in(file("dataset/review_Toronto.json"))
review.df <- flatten(review)
# only keep stars, businees id and user id in the review dataset
review.df <- review.df[,c("business_id", "user_id", "stars")]
review.df$categories <- business.df$categories [match(review.df$business_id, business.df$business_id)]
review.df
# drop null or empty neighborhood
review.df <- review.df[!(is.na(review.df) | review.df==""), ]
review.df
# one-hot-encode all the categories
library(qdapTools)
categories <- mtabulate(review.df$categories)
categories
# create a count dataframe to count the frequency of each unique category and then sum them
categories.sum <- data.frame(
category=character(),
counts=integer(),
stringsAsFactors=FALSE)
for (name in names(categories)) {
categories.sum[nrow(categories.sum) + 1,] = list(name, sum(categories[name]))
}
categories.sum
# Keep the top 15 categories
eliminated.categories <- categories.sum[order(categories.sum$counts, decreasing = TRUE),]
eliminated.categories <- eliminated.categories[1:15,]
eliminated.categories
# remove all the column that is not the top 15 category
library(dplyr)
categories.filtered <- categories[eliminated.categories$category]
categories.filtered$ID <- seq.int(nrow(categories.filtered))
review.df$ID <- seq.int(nrow(review.df))
encoded.review.df <- left_join(business.df[,c("ID", "business_id", "name", "stars")], categories.filtered)
library("jsonlite")
library("ggplot2")
library("readr")
setwd("~/Documents/Project/YelpAnalysis/")
business <- stream_in(file( "dataset/business_open_Toronto.json"))
business.df <- flatten(business)
business.df
business.df <- business.df[!(is.na(business.df$neighborhood) | business.df$neighborhood==""), ]
business.df
# one-hot-encode all the categories
library(qdapTools)
categories <- mtabulate(business.df$categories)
categories
fit2 <- blca.em(encoded.business.df[, -1], 25)
library("jsonlite")
library("ggplot2")
library("readr")
setwd("~/Documents/Project/YelpAnalysis/")
business <- stream_in(file( "dataset/business_open_Toronto.json"))
business.df <- flatten(business)
business.df
business.df <- business.df[!(is.na(business.df$neighborhood) | business.df$neighborhood==""), ]
business.df
# one-hot-encode all the categories
library(qdapTools)
categories <- mtabulate(business.df$categories)
categories
# create a count dataframe to count the frequency of each unique category and then sum them
categories.sum <- data.frame(
category=character(),
counts=integer(),
stringsAsFactors=FALSE)
for (name in names(categories)) {
categories.sum[nrow(categories.sum) + 1,] = list(name, sum(categories[name]))
}
categories.sum
# Keep the top 15 categories
eliminated.categories <- categories.sum[order(categories.sum$counts, decreasing = TRUE),]
eliminated.categories <- eliminated.categories[1:15,]
eliminated.categories
# remove all the column that is not the top 15 category
library(dplyr)
categories.filtered <- categories[eliminated.categories$category]
categories.filtered
categories.filtered$ID <- seq.int(nrow(categories.filtered))
business.df$ID <- seq.int(nrow(business.df))
encoded.business.df <- left_join(business.df[,c("ID", "neighborhood")], categories.filtered)
encoded.business.df
# drop unneeded columns
drops <- c("ID")
encoded.business.df <- encoded.business.df[ , !(names(encoded.business.df) %in% drops)]
encoded.business.df
# label neighborhood
encoded.business.df$neighborhood <- factor(encoded.business.df$neighborhood)
encoded.business.df$neighborhood <- as.numeric(encoded.business.df$neighborhood)
encoded.business.df
encoded.business.df[,-1]
fit2 <- blca.em(encoded.business.df[, -1], 5)
library(BayesLCA)
fit2 <- blca.em(encoded.business.df[, -1], 5)
fit2 <- blca.em(encoded.business.df[, -1], 5)
fit2$BIC
Zscore(encoded.business.df[, -1], fit2)
Zscore(encoded.business.df[, -1], fit2)
MAP(Zscore(encoded.business.df[, -1], fit2))
MAP(Zscore(encoded.business.df$neighborhood, fit2))
MAP(Zscore(encoded.business.df[, 1], fit2))
table(MAP(Zscore(encoded.business.df[, -1], fit2)), encoded.business.df$neighborhood)
for(i in 1:20) {
fit2 <- blca.em(encoded.business.df, i*5)
ClusterNum[i-1] <- i*5
BIC[i-1] <- fit2$BIC
}
ClusterNum <- list()
BIC <- list()
for(i in 1:20) {
fit2 <- blca.em(encoded.business.df, i*5)
ClusterNum[i-1] <- i*5
BIC[i-1] <- fit2$BIC
}
for(i in 1:20) {
fit2 <- blca.em(encoded.business.df, i*5)
ClusterNum[i-1] <- i*5
BIC[i-1] <- fit2$BIC
}
data.frame(ClusterNum, BIC)
do.call(rbind, Map(data.frame, ClusterNum=ClusterNum, BIC=BIC))
table <- do.call(rbind, Map(data.frame, ClusterNum=ClusterNum, BIC=BIC))
View(table)
fit2 <- blca.em(encoded.business.df[, -1], 40)
fit2 <- blca.em(encoded.business.df[, -1], 40)
fit2 <- blca.em(encoded.business.df[, -1], 40)
fit2 <- blca.em(encoded.business.df[, -1], 40)
sapply(business.df, class)
sapply(business.df, class)
require(SparkR)
sapply(business.df, class)
printSchema(business.df)
sapply(business.df, class)
sapply(business.df, class)
max(fit$params[,1])
library("jsonlite")
library("ggplot2")
library("readr")
setwd("~/Documents/Project/YelpAnalysis/")
# Import business dataset
business <- stream_in(file( "dataset/business.json"))
business.df <- flatten(business)
business.df
sapply(business.df, class)
# Import review dataset
review <- stream_in(file("dataset/review_Toronto.json"))
review.df <- flatten(review)
review.df
# only keep stars, businees id and user id in the review dataset
review.df <- review.df[ ,c("business_id", "user_id", "stars")]
fit2$BIC
table(MAP(Zscore(encoded.business.df[, -1], fit2)), encoded.business.df$neighborhood)
plot(table(MAP(Zscore(encoded.business.df[, -1], fit2)), encoded.business.df$neighborhood))
plot(table(MAP(Zscore(encoded.business.df[, -1], fit2)), encoded.business.df$neighborhood), which = 1)
plot(fit2, which = 1)
